{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec6cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchvision=1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a0a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.utils.parametrizations\n",
    "# Define the device to use for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ecf8d5",
   "metadata": {},
   "source": [
    "# 2.\tDefine the device: We define the device to use for training. If a GPU is available, we use it, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f88956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccbf9a",
   "metadata": {},
   "source": [
    "# 3.\tDefine the transforms: We define a series of transformations to be applied to the images in the dataset. These transformations resize the images to 64x64 pixels, center crop them, convert them to tensors, and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a11a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "dataset = datasets.CelebA(root='./data', split='train', transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e27e16",
   "metadata": {},
   "source": [
    "# 4.\tDefine the dataset: We define the CelebA dataset, specifying the path to the dataset, the split to use (train), the transform to apply to the images, and whether to download the dataset if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acc5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1790b0",
   "metadata": {},
   "source": [
    "# 5.\tDefine the dataloader: We define a dataloader that will be used to iterate over the dataset in batches during training. The dataloader takes the dataset as input, along with the batch size, whether to shuffle the dataset, and the number of workers to use for loading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4a4a0",
   "metadata": {},
   "source": [
    "# 6.\tDefine the Self-Attention module: We define a module that implements self-attention, which will be used in the generator network. The self-attention module takes as input a tensor and returns a tensor with attention applied. The module consists of three convolutional layers for computing the query, key, and value, followed by a gamma parameter for scaling the attention output, and a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc92030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Self-Attention module\n",
    "class SelfAttentionCnn(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttentionCnn, self).__init__()\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=1)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Compute the query, key, and value tensors\n",
    "        query = self.query_conv(x).view(batch_size, -1, height*width).permute(0, 2, 1)\n",
    "        key = self.key_conv(x).view(batch_size, -1, height*width)\n",
    "        value = self.value_conv(x).view(batch_size, -1, height*width)\n",
    "\n",
    "        # Compute the attention map\n",
    "        energy = torch.bmm(query, key)\n",
    "        attention = self.softmax(energy)\n",
    "\n",
    "        # Apply attention to the value tensor\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "\n",
    "        # Scale the output tensor by learnable parameter gamma and apply to input tensor\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b80b7",
   "metadata": {},
   "source": [
    "# The Self-Attention layer is a key component of the Self-Attention GAN model. It helps the generator to focus on important parts of the image and generate high-quality images.\n",
    "\n",
    "#Next, the generator network is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        #spectral normalization (SN) is used as proposed by Miyato et al. (Miyato et al., 2018)\n",
    "        \n",
    "        self.linear = nn.Linear(z_dim, 4*4*1024)\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4,stride=2, padding=1))\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1))\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.self_attention = SelfAttentionCnn(in_channels=256)\n",
    "        self.conv3 = nn.utils.spectral_norm(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.conv4 = nn.utils.spectral_norm(nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4,stride=2, padding=1))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear(z)\n",
    "        out = out.view(-1, 1024, 4, 4)\n",
    "        out = self.relu(self.bn1(self.conv1(out)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.self_attention(out)\n",
    "        out = self.relu(self.bn3(self.conv3(out)))\n",
    "        out = self.tanh(self.conv4(out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f36ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        #spectral normalization (SN) is used as proposed by Miyato et al. (Miyato et al., 2018)\n",
    "        \n",
    "        self.conv1 =  nn.utils.spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1))\n",
    "        self.conv2 =  nn.utils.spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 =  nn.utils.spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4,stride=2, padding=1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.self_attention = SelfAttentionCnn(in_channels=256)\n",
    "        self.conv4 = nn.utils.spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4,stride=2, padding=1))\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 =  nn.utils.spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add leaky ReLU as used in original paper\n",
    "        out = self.leaky_relu(self.conv1(x))\n",
    "        out = self.leaky_relu(self.bn2(self.conv2(out)))\n",
    "        out = self.leaky_relu(self.bn3(self.conv3(out)))\n",
    "        out = self.self_attention(out)\n",
    "        out = self.leaky_relu(self.bn4(self.conv4(out)))\n",
    "        out = self.sigmoid(self.conv5(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74450af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the learning rate\n",
    "lr = 0.0002\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 200\n",
    "\n",
    "# Define the generator and discriminator networks\n",
    "z_dim = 100\n",
    "G = Generator(z_dim=z_dim).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Define the optimizer for the generator and discriminator networks\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3ffdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to train the networks\n",
    "def train(G, D, train_loader, criterion, G_optimizer, D_optimizer, z_dim, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_imag, _) in enumerate(train_loader):\n",
    "            batch_size = real_imag.size(0)\n",
    "            real_imag = real_imag.to(device)\n",
    "\n",
    "            # Training the Discriminator with real images\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            outputs = D(real_imag)\n",
    "            D_loss_real = criterion(outputs, real_labels)\n",
    "            \n",
    "             # Train discriminator with fake images\n",
    "            noise = torch.randn(batch_size, z_dim, device=device)\n",
    "            fake_imag = generator(noise)\n",
    "            fake_labels = torch.zeros(batch_size, device=device)\n",
    "            fake_logits = discriminator(fake_imag.detach())\n",
    "            d_loss_fake = criterion(fake_logits, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "            noise = torch.randn(batch_size, z_dim, device=device)\n",
    "            fake_imag = generator(noise)\n",
    "            fake_labels = torch.ones(batch_size, device=device)\n",
    "            fake_logits = discriminator(fake_imag)\n",
    "            g_loss = criterion(fake_logits, fake_labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # Print losses and save sample images periodically\n",
    "            if i % 100 == 0:\n",
    "                  print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(dataloader)}] D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c27e74",
   "metadata": {},
   "source": [
    "# The train function takes as input the generator, discriminator, dataloader, num_epochs, device, and lr. \n",
    "#It initializes the loss functions and optimizers and generates a fixed set of noise vectors for visualization. \n",
    "#It then loops over the specified number of epochs and the batches in the dataloader.\n",
    "#For each batch, it trains the discriminator with real and fake images and the generator with fake images. \n",
    "#It then prints the losses and saves sample images periodically. \n",
    "#Finally, it returns the trained generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b66850a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1949702139.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [11]\u001b[1;36m\u001b[0m\n\u001b[1;33m    with torch.no_grad():\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Save sample images\n",
    "    with torch.no_grad():\n",
    "        fake_imag = generator(fixed_noise).detach().cpu()\n",
    "         save_image(fake_imag, f\"sample_images/epoch_{epoch+1}.png\", normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cd50f146da535ab3830ed5f78683553ef57acd3032c46b9f95757dd92f9c813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
