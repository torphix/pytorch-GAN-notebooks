{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "import zipfile\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from livelossplot import PlotLosses\n",
    "from IPython.display import clear_output\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def load_data():\n",
    "    # Download data if not present\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    if not os.path.exists(\"data/summer2winter_yosemite.zip\"):\n",
    "        print(\"Downloading data...\")\n",
    "        os.system(\n",
    "            \"wget http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/summer2winter_yosemite.zip --directory-prefix=data\"\n",
    "        )\n",
    "    if len(os.listdir(\"data\")) == 1:\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile(\"data/summer2winter_yosemite.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data\")\n",
    "\n",
    "\n",
    "class UnpairedImageDataset(Dataset):\n",
    "    def __init__(self, root_A_dir, root_B_dir):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "        self.files_A = sorted(\n",
    "            [os.path.join(root_A_dir, x) for x in os.listdir(root_A_dir)]\n",
    "        )\n",
    "        self.files_B = sorted(\n",
    "            [os.path.join(root_B_dir, x) for x in os.listdir(root_B_dir)]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10#min(len(self.files_A), len(self.files_B))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_A = self.transform(Image.open(self.files_A[index]))\n",
    "        img_B = self.transform(Image.open(self.files_B[index]))\n",
    "        return img_A, img_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator, adopts the UNET architecture\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, emb_d: int, n_down_blocks: int, n_res_blocks: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, emb_d, 7, 1, 3),\n",
    "            nn.BatchNorm2d(emb_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for _ in range(n_down_blocks):\n",
    "            self.down_blocks.append(DownBlock(emb_d, emb_d * 2))\n",
    "            emb_d *= 2\n",
    "        self.res_blocks = nn.Sequential(*[ResBlock(emb_d) for _ in range(n_res_blocks)])\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for _ in range(n_down_blocks):\n",
    "            self.up_blocks.append(UpBlock(emb_d, emb_d // 2))\n",
    "            emb_d //= 2\n",
    "\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(emb_d, 3, 3, 1, 1), nn.Tanh())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_conv(x)\n",
    "        # Down\n",
    "        latents = []\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x)\n",
    "            latents.append(x)\n",
    "        # Res\n",
    "        x = self.res_blocks(x)\n",
    "        # Up\n",
    "        latents.reverse()\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            x = up_block(x + latents[i])\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_d, out_d, 3, 2, 1),\n",
    "            nn.BatchNorm2d(out_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_d, in_d, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_d, in_d, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        res = x\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_d, out_d, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(out_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator, adopts the PatchGAN architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_d: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        self.in_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, emb_d, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    ResBlock(emb_d),\n",
    "                    nn.Conv2d(emb_d, emb_d * 2, 4, 2, 1),\n",
    "                )\n",
    "            )\n",
    "            emb_d *= 2\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(emb_d, 1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        data_dir: str,\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        lr: float,\n",
    "        beta_1: float,\n",
    "        beta_2: float,\n",
    "        # Model parameters\n",
    "        emb_d: int,\n",
    "        n_down_blocks: int,\n",
    "        n_res_blocks: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        data_dir: Directory containing the data, must contain trainA and trainB folders\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta_1: Beta 1 for Adam optimiser\n",
    "        beta_2: Beta 2 for Adam optimiser\n",
    "        \"\"\"\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        # Converts original image to stylised image\n",
    "        self.generator_A2B = Generator(emb_d, n_down_blocks, n_res_blocks)\n",
    "        # Converts stylised image back to original image\n",
    "        self.generator_B2A = Generator(emb_d, n_down_blocks, n_res_blocks)\n",
    "        # Discriminator for original image\n",
    "        self.discriminator_A = Discriminator()\n",
    "        # Discriminator for stylised image\n",
    "        self.discriminator_B = Discriminator()\n",
    "        # Optimisers\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            list(self.generator_A2B.parameters())\n",
    "            + list(self.generator_B2A.parameters()),\n",
    "            lr=lr,\n",
    "            betas=(beta_1, beta_2),\n",
    "        )\n",
    "        self.optim_D_A = torch.optim.Adam(\n",
    "            self.discriminator_A.parameters(), lr=lr, betas=(beta_1, beta_2)\n",
    "        )\n",
    "        self.optim_D_B = torch.optim.Adam(\n",
    "            self.discriminator_B.parameters(), lr=lr, betas=(beta_1, beta_2)\n",
    "        )\n",
    "        # Dataloaders\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/trainA\"\n",
    "        ), f\"Data directory {data_dir}/trainA does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/trainB\"\n",
    "        ), f\"Data directory {data_dir}/trainB does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/testA\"\n",
    "        ), f\"Data directory {data_dir}/testA does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/testB\"\n",
    "        ), f\"Data directory {data_dir}/testB does not exist\"\n",
    "        train_dataset = UnpairedImageDataset(f\"{data_dir}/trainA\", f\"{data_dir}/trainB\")\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_dataset = UnpairedImageDataset(f\"{data_dir}/testA\", f\"{data_dir}/testB\")\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        print(self.test_loader.__len__())\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        # Cast to device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator_A2B.to(self.device)\n",
    "        self.generator_B2A.to(self.device)\n",
    "        self.discriminator_A.to(self.device)\n",
    "        self.discriminator_B.to(self.device)\n",
    "        # Init variables\n",
    "        self.global_step = 0\n",
    "        # self.plotlosses = PlotLosses()\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "\n",
    "    def train_iter(self, epoch:int):\n",
    "        # Set model to training mode\n",
    "        self.generator_A2B.train()\n",
    "        self.generator_B2A.train()\n",
    "        self.discriminator_A.train()\n",
    "        self.discriminator_B.train()\n",
    "        # Train on a batch of images\n",
    "        for i, (img_A, img_B) in enumerate(\n",
    "            tqdm(self.train_loader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            img_A, img_B = self.to_device(img_A, img_B)\n",
    "            # Train the generators\n",
    "            self.optim_G.zero_grad()\n",
    "            # Compute the loss\n",
    "            loss_G = self.compute_generator_loss(img_A, img_B)\n",
    "            # Backpropagate\n",
    "            loss_G.backward()\n",
    "            # Update the weights\n",
    "            self.optim_G.step()\n",
    "            # Train the discriminators\n",
    "            self.optim_D_A.zero_grad()\n",
    "            self.optim_D_B.zero_grad()\n",
    "            # Compute the loss\n",
    "            loss_D_A = self.compute_discriminator_loss(\n",
    "                img_A, self.generator_B2A(img_B), self.discriminator_A\n",
    "            )\n",
    "            loss_D_B = self.compute_discriminator_loss(\n",
    "                img_B, self.generator_A2B(img_A), self.discriminator_B\n",
    "            )\n",
    "            # Backpropagate\n",
    "            loss_D_A.backward()\n",
    "            loss_D_B.backward()\n",
    "            # Update the weights\n",
    "            self.optim_D_A.step()\n",
    "            self.optim_D_B.step()\n",
    "            # Log the losses\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                #     self.plotlosses.update(\n",
    "                #         {\"loss_G\": loss_G.item(), \"loss_D_A\": loss_D_A.item(), \"loss_D_B\": loss_D_B.item()}\n",
    "                #     )\n",
    "                #     self.plotlosses.send()\n",
    "                self.log_image(\n",
    "                    self.generator_A2B(img_A), f\"{self.global_step}_img_A_generated.png\"\n",
    "                )\n",
    "                self.log_image(img_A, f\"{self.global_step}_img_A.png\")\n",
    "\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def compute_generator_loss(self, img_A, img_B) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the generator loss for CycleGAN\n",
    "        this is the loss for both generators ie: A -> B and B -> A\n",
    "        \"\"\"\n",
    "        # A -> B\n",
    "        # Generate the stylised image\n",
    "        generated_img_B = self.generator_A2B(img_A)\n",
    "        # Reconstruct the original image\n",
    "        generated_img_A = self.generator_B2A(generated_img_B)\n",
    "        # Pass generated image through discriminator\n",
    "        pred_D_B = self.discriminator_B(generated_img_B)\n",
    "        # Compute the losses\n",
    "        recon_loss = F.l1_loss(generated_img_A, img_A)  # Reconstruction loss\n",
    "        adv_loss = F.mse_loss(pred_D_B, torch.ones_like(pred_D_B))  # Adversarial loss\n",
    "        # B -> A\n",
    "        generated_img_A = self.generator_B2A(img_B)\n",
    "        generated_img_B = self.generator_A2B(generated_img_A)\n",
    "        pred_D_A = self.discriminator_A(generated_img_A)\n",
    "        recon_loss += F.l1_loss(generated_img_B, img_B)\n",
    "        adv_loss += F.mse_loss(pred_D_A, torch.ones_like(pred_D_A))\n",
    "        # Return the total loss\n",
    "        return recon_loss + adv_loss\n",
    "\n",
    "    def compute_discriminator_loss(\n",
    "        self,\n",
    "        real_img: torch.Tensor,\n",
    "        generated_img: torch.Tensor,\n",
    "        discriminator: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for each discriminator (A & B)\n",
    "        this consists of the loss for the real images and the fake images\n",
    "        \"\"\"\n",
    "        real_D = discriminator(real_img)\n",
    "        fake_D = discriminator(generated_img)\n",
    "        real_loss = F.mse_loss(real_D, torch.ones_like(real_D))\n",
    "        fake_loss = F.mse_loss(fake_D, torch.zeros_like(fake_D))\n",
    "        # Compute the losses\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"global_step\": self.global_step,\n",
    "                \"generator_A2B\": self.generator_A2B.state_dict(),\n",
    "                \"generator_B2A\": self.generator_B2A.state_dict(),\n",
    "                \"discriminator_A\": self.discriminator_A.state_dict(),\n",
    "                \"discriminator_B\": self.discriminator_B.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                \"optim_D_A\": self.optim_D_A.state_dict(),\n",
    "                \"optim_D_B\": self.optim_D_B.state_dict(),\n",
    "            },\n",
    "            f\"results/checkpoint_{self.global_step}.pth\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.global_step = checkpoint[\"global_step\"]\n",
    "        self.generator_A2B.load_state_dict(checkpoint[\"generator_A2B\"])\n",
    "        self.generator_B2A.load_state_dict(checkpoint[\"generator_B2A\"])\n",
    "        self.discriminator_A.load_state_dict(checkpoint[\"discriminator_A\"])\n",
    "        self.discriminator_B.load_state_dict(checkpoint[\"discriminator_B\"])\n",
    "        self.optim_G.load_state_dict(checkpoint[\"optim_G\"])\n",
    "        self.optim_D_A.load_state_dict(checkpoint[\"optim_D_A\"])\n",
    "        self.optim_D_B.load_state_dict(checkpoint[\"optim_D_B\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            \"results/logs\" + name,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 10.76 GiB total capacity; 9.39 GiB already allocated; 20.69 MiB free; 9.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m load_data()\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     n_ckpt_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m      4\u001b[0m     n_log_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     flush_prev_logs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[6], line 100\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iter(e)\n",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m, in \u001b[0;36mTrainer.train_iter\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m loss_G \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_generator_loss(img_A, img_B)\n\u001b[1;32m    117\u001b[0m \u001b[39m# Backpropagate\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m loss_G\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    119\u001b[0m \u001b[39m# Update the weights\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_G\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/styleGAN/venv/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/styleGAN/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 10.76 GiB total capacity; 9.39 GiB already allocated; 20.69 MiB free; 9.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "load_data()\n",
    "trainer = Trainer(\n",
    "    n_ckpt_steps=1000,\n",
    "    n_log_steps=1000,\n",
    "    epochs=10000,\n",
    "    data_dir=\"data/summer2winter_yosemite\",\n",
    "    batch_size=4,\n",
    "    lr=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    emb_d=32,\n",
    "    n_down_blocks=2,\n",
    "    n_res_blocks=2,\n",
    "    flush_prev_logs=True,\n",
    ")\n",
    "trainer.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95b6abaacb1982205bfc4504813087e12c147c61617611e95b3465de0a150de6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
