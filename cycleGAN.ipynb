{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import zipfile\n",
    "import itertools\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def load_data():\n",
    "    # Download data if not present\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    if not os.path.exists(\"data/summer2winter_yosemite.zip\"):\n",
    "        print(\"Downloading data...\")\n",
    "        os.system(\n",
    "            \"wget http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/summer2winter_yosemite.zip --directory-prefix=data\"\n",
    "        )\n",
    "    if len(os.listdir(\"data\")) == 1:\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile(\"data/summer2winter_yosemite.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\"data\")\n",
    "\n",
    "\n",
    "class UnpairedImageDataset(Dataset):\n",
    "    def __init__(self, root_A_dir, root_B_dir):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "        self.files_A = sorted(\n",
    "            [os.path.join(root_A_dir, x) for x in os.listdir(root_A_dir)]\n",
    "        )\n",
    "        self.files_B = sorted(\n",
    "            [os.path.join(root_B_dir, x) for x in os.listdir(root_B_dir)]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10#min(len(self.files_A), len(self.files_B))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_A = self.transform(Image.open(self.files_A[index]))\n",
    "        img_B = self.transform(Image.open(self.files_B[index]))\n",
    "        return img_A, img_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator, adopts the UNET architecture\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, emb_d: int, n_down_blocks: int, n_res_blocks: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, emb_d, 7, 1, 3),\n",
    "            nn.BatchNorm2d(emb_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for _ in range(n_down_blocks):\n",
    "            self.down_blocks.append(DownBlock(emb_d, emb_d * 2))\n",
    "            emb_d *= 2\n",
    "        self.res_blocks = nn.Sequential(*[ResBlock(emb_d) for _ in range(n_res_blocks)])\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for _ in range(n_down_blocks):\n",
    "            self.up_blocks.append(UpBlock(emb_d*2, emb_d//2))\n",
    "            emb_d //= 2\n",
    "\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(emb_d, 3, 3, 1, 1), nn.Tanh())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_conv(x)\n",
    "        # Down\n",
    "        latents = []\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x)\n",
    "            latents.append(x)\n",
    "        # Res\n",
    "        x = self.res_blocks(x)\n",
    "        # Up\n",
    "        latents.reverse()\n",
    "        for i, up_block in enumerate(self.up_blocks):\n",
    "            x = up_block(torch.cat([x, latents[i]], dim=1))\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_d, out_d, 3, 2, 1),\n",
    "            nn.InstanceNorm2d(out_d),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_d, in_d, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_d, in_d, 3, 1, 1),\n",
    "            nn.BatchNorm2d(in_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        res = x\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_d, out_d, 3, 2, 1, 1),\n",
    "            nn.InstanceNorm2d(out_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator, adopts the PatchGAN architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, emb_d: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        self.in_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, emb_d, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(emb_d, emb_d*2, 4, 2, 1),\n",
    "                    nn.InstanceNorm2d(emb_d*2),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "            )\n",
    "            emb_d *= 2\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(emb_d, 1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_conv(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        data_dir: str,\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        lr: float,\n",
    "        beta_1: float,\n",
    "        beta_2: float,\n",
    "        # Model parameters\n",
    "        emb_d: int,\n",
    "        n_down_blocks: int,\n",
    "        n_res_blocks: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        data_dir: Directory containing the data, must contain trainA and trainB folders\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta_1: Beta 1 for Adam optimiser\n",
    "        beta_2: Beta 2 for Adam optimiser\n",
    "        \"\"\"\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        # Converts original image to stylised image\n",
    "        self.generator_A2B = Generator(emb_d, n_down_blocks, n_res_blocks)\n",
    "        # Converts stylised image back to original image\n",
    "        self.generator_B2A = Generator(emb_d, n_down_blocks, n_res_blocks)\n",
    "        # Discriminator for original image\n",
    "        self.discriminator_A = Discriminator()\n",
    "        # Discriminator for stylised image\n",
    "        self.discriminator_B = Discriminator()\n",
    "        # Optimisers\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            itertools.chain(\n",
    "                self.generator_A2B.parameters(), self.generator_B2A.parameters()\n",
    "            ),\n",
    "            lr=lr,\n",
    "            betas=(beta_1, beta_2),\n",
    "        )\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            itertools.chain(\n",
    "                self.discriminator_A.parameters(), self.discriminator_B.parameters()\n",
    "            ),\n",
    "            lr=lr,\n",
    "            betas=(beta_1, beta_2),\n",
    "        )\n",
    "        # Dataloaders\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/trainA\"\n",
    "        ), f\"Data directory {data_dir}/trainA does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/trainB\"\n",
    "        ), f\"Data directory {data_dir}/trainB does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/testA\"\n",
    "        ), f\"Data directory {data_dir}/testA does not exist\"\n",
    "        assert os.path.exists(\n",
    "            f\"{data_dir}/testB\"\n",
    "        ), f\"Data directory {data_dir}/testB does not exist\"\n",
    "        train_dataset = UnpairedImageDataset(f\"{data_dir}/trainA\", f\"{data_dir}/trainB\")\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_dataset = UnpairedImageDataset(f\"{data_dir}/testA\", f\"{data_dir}/testB\")\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        # Cast to device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator_A2B.to(self.device)\n",
    "        self.generator_B2A.to(self.device)\n",
    "        self.discriminator_A.to(self.device)\n",
    "        self.discriminator_B.to(self.device)\n",
    "        # Init variables\n",
    "        self.global_step = 0\n",
    "        # self.plotlosses = PlotLosses()\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "\n",
    "    def train_iter(self, epoch: int):\n",
    "        # Set model to training mode\n",
    "        self.generator_A2B.train()\n",
    "        self.generator_B2A.train()\n",
    "        self.discriminator_A.train()\n",
    "        self.discriminator_B.train()\n",
    "        # Train on a batch of images\n",
    "        for i, (img_A, img_B) in enumerate(\n",
    "            tqdm(self.train_loader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            img_A, img_B = self.to_device(img_A, img_B)\n",
    "            # Train the generators\n",
    "            self.optim_G.zero_grad()\n",
    "            loss_G = self.compute_generator_loss(img_A, img_B)\n",
    "            loss_G.backward()\n",
    "            self.optim_G.step()\n",
    "            # Train the discriminators\n",
    "            self.optim_D.zero_grad()\n",
    "            loss_D_A = self.compute_discriminator_loss(\n",
    "                img_A, self.generator_B2A(img_B), self.discriminator_A\n",
    "            )\n",
    "            loss_D_B = self.compute_discriminator_loss(\n",
    "                img_B, self.generator_A2B(img_A), self.discriminator_B\n",
    "            )\n",
    "            loss_D = loss_D_A + loss_D_B\n",
    "            loss_D.backward()\n",
    "            self.optim_D.step()\n",
    "            # Log the losses\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                #     self.plotlosses.update(\n",
    "                #         {\"loss_G\": loss_G.item(), \"loss_D_A\": loss_D_A.item(), \"loss_D_B\": loss_D_B.item()}\n",
    "                #     )\n",
    "                #     self.plotlosses.send()\n",
    "                self.log_image(\n",
    "                    self.generator_A2B(img_A), f\"{self.global_step}_img_A_generated.png\"\n",
    "                )\n",
    "                self.log_image(img_A, f\"{self.global_step}_img_A.png\")\n",
    "\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def compute_generator_loss(self, img_A, img_B) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the generator loss for CycleGAN\n",
    "        this is the loss for both generators ie: A -> B and B -> A\n",
    "        \"\"\"\n",
    "        # A -> B\n",
    "        # Generate the stylised image\n",
    "        generated_img_B = self.generator_A2B(img_A)\n",
    "        # Reconstruct the original image\n",
    "        generated_img_A = self.generator_B2A(generated_img_B)\n",
    "        # Pass generated image through discriminator\n",
    "        pred_D_B = self.discriminator_B(generated_img_B)\n",
    "        # Compute the losses\n",
    "        recon_loss = F.l1_loss(generated_img_A, img_A)  # Reconstruction loss\n",
    "        adv_loss = F.mse_loss(pred_D_B, torch.ones_like(pred_D_B))  # Adversarial loss\n",
    "        # B -> A\n",
    "        generated_img_A = self.generator_B2A(img_B)\n",
    "        generated_img_B = self.generator_A2B(generated_img_A)\n",
    "        pred_D_A = self.discriminator_A(generated_img_A)\n",
    "        recon_loss += F.l1_loss(generated_img_B, img_B)\n",
    "        adv_loss += F.mse_loss(pred_D_A, torch.ones_like(pred_D_A))\n",
    "        # Return the total loss\n",
    "        return recon_loss + adv_loss\n",
    "\n",
    "    def compute_discriminator_loss(\n",
    "        self,\n",
    "        real_img: torch.Tensor,\n",
    "        generated_img: torch.Tensor,\n",
    "        discriminator: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for each discriminator (A & B)\n",
    "        this consists of the loss for the real images and the fake images\n",
    "        \"\"\"\n",
    "        real_D = discriminator(real_img)\n",
    "        fake_D = discriminator(generated_img)\n",
    "        real_loss = F.mse_loss(real_D, torch.ones_like(real_D))\n",
    "        fake_loss = F.mse_loss(fake_D, torch.zeros_like(fake_D))\n",
    "        # Compute the losses\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"global_step\": self.global_step,\n",
    "                \"generator_A2B\": self.generator_A2B.state_dict(),\n",
    "                \"generator_B2A\": self.generator_B2A.state_dict(),\n",
    "                \"discriminator_A\": self.discriminator_A.state_dict(),\n",
    "                \"discriminator_B\": self.discriminator_B.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                    \"optim_D\": self.optim_D.state_dict(),\n",
    "            },\n",
    "            f\"results/checkpoint_{self.global_step}.pth\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.global_step = checkpoint[\"global_step\"]\n",
    "        self.generator_A2B.load_state_dict(checkpoint[\"generator_A2B\"])\n",
    "        self.generator_B2A.load_state_dict(checkpoint[\"generator_B2A\"])\n",
    "        self.discriminator_A.load_state_dict(checkpoint[\"discriminator_A\"])\n",
    "        self.discriminator_B.load_state_dict(checkpoint[\"discriminator_B\"])\n",
    "        self.optim_G.load_state_dict(checkpoint[\"optim_G\"])\n",
    "        self.optim_D.load_state_dict(checkpoint[\"optim_D\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            \"results/logs\" + name,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()\n",
    "trainer = Trainer(\n",
    "    n_ckpt_steps=1000,\n",
    "    n_log_steps=1000,\n",
    "    epochs=10000,\n",
    "    data_dir=\"data/summer2winter_yosemite\",\n",
    "    batch_size=4,\n",
    "    lr=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    emb_d=64,\n",
    "    n_down_blocks=4,\n",
    "    n_res_blocks=1,\n",
    "    flush_prev_logs=True,\n",
    ")\n",
    "\n",
    "# Check model code\n",
    "# Check train code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39mrun()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95b6abaacb1982205bfc4504813087e12c147c61617611e95b3465de0a150de6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
