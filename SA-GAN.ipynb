{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load celebA dataset\n",
    "def get_dataloader(batch_size, shuffle, n_workers, image_size):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_size),\n",
    "        torchvision.transforms.CenterCrop(image_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CelebA(root='./data', transform=transform, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionConv(nn.Module):\n",
    "    def __init__(self, in_d, downscale_factor=8):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Downscale factor is suggested in the paper to reduce memory consumption\n",
    "        they use 8, but you can use 4 or 2 if you have enough memory\n",
    "        \"\"\"\n",
    "        self.downscale_factor = downscale_factor\n",
    "        self.k_conv = nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        self.q_conv = nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        self.v_conv = nn.Conv2d(in_d, in_d, 1, 1, 0)\n",
    "        # gamma is a learnable parameter (used in original paper)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.Conv2d(in_d, in_d, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_d, h, w)\n",
    "        \"\"\"\n",
    "        batch_size, in_d, h, w = x.shape\n",
    "        # Embed input and reshape for matrix multiplication\n",
    "        k = self.k_conv(x).view(batch_size, -1, h * w)\n",
    "        q = self.q_conv(x).view(batch_size, -1, h * w)\n",
    "        v = self.v_conv(x).view(batch_size, -1, h * w)\n",
    "        # (batch_size, h * w, h * w)\n",
    "        attn = torch.bmm(k.transpose(-2, -1), q)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # (batch_size, in_d, h * w)\n",
    "        out = torch.bmm(v, attn.transpose(-2, -1))\n",
    "        out = out.view(batch_size, in_d, h, w)\n",
    "        out = self.out_conv(out)\n",
    "        # Add residual connection and scale by learnable parameter gamma\n",
    "        out = self.gamma * out + x\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "# Generator, DC-GAN with self attention\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=64, target_output_size=256, emb_dim=1024):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.Conv2d(noise_dim, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        n_layers = int(np.log2(target_output_size))\n",
    "        for _ in range(n_layers-1):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        emb_dim,\n",
    "                        emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            # Don't shrink emb_dim below too low\n",
    "            if emb_dim // 2 >= 64:\n",
    "                emb_dim = emb_dim // 2\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        # Self attention sandwhiches the penultimate layer\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "        self.penultimate_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(emb_dim, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(nn.Conv2d(emb_dim, 3, 3, 1, 1), nn.Tanh())\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.in_layer(noise)\n",
    "        # Main layers\n",
    "        out = self.layers(out)\n",
    "        # Attention layers\n",
    "        out, _ = self.self_attn_1(out)\n",
    "        out = self.penultimate_layer(out)\n",
    "        out, _ = self.self_attn_2(out)\n",
    "        # Out layers\n",
    "        out = self.out_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Discriminator, adopts the PatchGAN architecture ie: output is a receptive field of n x n\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_d=3, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_d, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        emb_dim,\n",
    "                        emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            # Don't shrink emb_dim below too low\n",
    "            if emb_dim // 2 >= 64:\n",
    "                emb_dim = emb_dim // 2\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        # Self attention sandwhiches the penultimate layer\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "        self.penultimate_layer = nn.Sequential(\n",
    "            nn.Conv2d(emb_dim, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(nn.Conv2d(emb_dim, 1, 3, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.in_layer(noise)\n",
    "        # Main layers\n",
    "        out = self.layers(out)\n",
    "        # Attention layers\n",
    "        out, _ = self.self_attn_1(out)\n",
    "        out = self.penultimate_layer(out)\n",
    "        out, _ = self.self_attn_2(out)\n",
    "        # Out layers\n",
    "        out = self.out_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        lr: float,\n",
    "        beta_1: float,\n",
    "        beta_2: float,\n",
    "        # Model parameters\n",
    "        noise_d: int,\n",
    "        emb_d: int,\n",
    "        output_size: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        data_dir: Directory containing the data, must contain trainA and trainB folders\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta_1: Beta 1 for Adam optimiser\n",
    "        beta_2: Beta 2 for Adam optimiser\n",
    "        \"\"\"\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = Generator(\n",
    "            noise_dim=noise_d, emb_dim=emb_d, target_output_size=output_size\n",
    "        ).to(self.device)\n",
    "        self.discriminator = Discriminator(in_d=3, emb_dim=emb_d).to(self.device)\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(beta_1, beta_2),\n",
    "        )\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(beta_1, beta_2),\n",
    "        )\n",
    "        self.train_dataloader = get_dataloader(batch_size, True, 4, output_size)\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_d = noise_d\n",
    "        # Init variables\n",
    "        self.global_step = 0\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "\n",
    "    def train_iter(self, epoch: int):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        # Train on a batch of images\n",
    "        for i, real_img in enumerate(\n",
    "            tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            # Train the generators\n",
    "            self.optim_G.zero_grad()\n",
    "            loss_G = self.compute_generator_loss()\n",
    "            loss_G.backward()\n",
    "            self.optim_G.step()\n",
    "            # Train the discriminators\n",
    "            self.optim_D.zero_grad()\n",
    "            noise = torch.randn((self.batch_size, 64, 1, 1)).to(self.device)\n",
    "            fake_image = self.generator(noise)\n",
    "            loss_D = self.compute_discriminator_loss(real_img, fake_image)\n",
    "            loss_D.backward()\n",
    "            self.optim_D.step()\n",
    "            # Log the losses\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                self.log_image(real_img, f\"{self.global_step}_real_image.png\")\n",
    "                self.log_image(\n",
    "                    self.generator(noise), f\"{self.global_step}_fake_image.png\"\n",
    "                )\n",
    "\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def compute_generator_loss(self) -> torch.Tensor:\n",
    "        noise_x = torch.randn(self.batch_size, self.noise_d, 1, 1).to(self.device)\n",
    "        fake_image = self.generator(noise_x)\n",
    "        fake_pred_D = self.discriminator(fake_image)\n",
    "        loss_G = F.mse_loss(fake_pred_D, torch.ones_like(fake_pred_D))\n",
    "        return loss_G\n",
    "\n",
    "    def compute_discriminator_loss(\n",
    "        self,\n",
    "        real_img: torch.Tensor,\n",
    "        generated_img: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        print(real_img.shape, generated_img.shape)\n",
    "        real_pred_D = self.discriminator(real_img)\n",
    "        real_loss_D = F.mse_loss(\n",
    "            real_pred_D, torch.ones_like(real_pred_D)\n",
    "        )\n",
    "        fake_pred_D = self.discriminator(generated_img)\n",
    "        fake_loss_D = F.mse_loss(\n",
    "            fake_pred_D, torch.zeros_like(fake_pred_D)\n",
    "        )\n",
    "        loss_D = (real_loss_D + fake_loss_D) / 2\n",
    "        return loss_D\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"generator\": self.generator.state_dict(),\n",
    "                \"discriminator\": self.discriminator.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                \"optim_D\": self.optim_D.state_dict(),\n",
    "            },\n",
    "            f\"results/checkpoints/{self.global_step}.pt\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        state_dict = torch.load(path)\n",
    "        self.generator.load_state_dict(state_dict[\"generator\"])\n",
    "        self.discriminator.load_state_dict(state_dict[\"discriminator\"])\n",
    "        self.optim_G.load_state_dict(state_dict[\"optim_G\"])\n",
    "        self.optim_D.load_state_dict(state_dict[\"optim_D\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            \"results/logs\" + name,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Trainer(\n\u001b[1;32m      2\u001b[0m     n_ckpt_steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     n_log_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.0002\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     beta_1\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     beta_2\u001b[39m=\u001b[39;49m\u001b[39m0.999\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     noise_d\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     emb_d\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     output_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m )\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[27], line 62\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m---> 62\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iter(e)\n",
      "Cell \u001b[0;32mIn[27], line 80\u001b[0m, in \u001b[0;36mTrainer.train_iter\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     78\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m64\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     79\u001b[0m fake_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(noise)\n\u001b[0;32m---> 80\u001b[0m loss_D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_discriminator_loss(real_img, fake_image)\n\u001b[1;32m     81\u001b[0m loss_D\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_D\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[27], line 107\u001b[0m, in \u001b[0;36mTrainer.compute_discriminator_loss\u001b[0;34m(self, real_img, generated_img)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_discriminator_loss\u001b[39m(\n\u001b[1;32m    103\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    104\u001b[0m     real_img: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    105\u001b[0m     generated_img: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    106\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mprint\u001b[39m(real_img\u001b[39m.\u001b[39;49mshape, generated_img\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m     real_pred_D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator(real_img)\n\u001b[1;32m    109\u001b[0m     real_loss_D \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(\n\u001b[1;32m    110\u001b[0m         real_pred_D, torch\u001b[39m.\u001b[39mones_like(real_pred_D)\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "Trainer(\n",
    "    n_ckpt_steps=1000,\n",
    "    n_log_steps=100,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    lr=0.0002,\n",
    "    beta_1=0.5,\n",
    "    beta_2=0.999,\n",
    "    noise_d=64,\n",
    "    emb_d=64,\n",
    "    output_size=64,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Add spectral norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95b6abaacb1982205bfc4504813087e12c147c61617611e95b3465de0a150de6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
