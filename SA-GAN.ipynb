{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (4.65.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (1.24.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in ./venv/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (56.0.0)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in ./venv/lib/python3.8/site-packages (0.15.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: torch==2.0.0 in ./venv/lib/python3.8/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.8/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (3.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (11.7.91)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.8/site-packages (from torch==2.0.0->torchvision) (2.14.3)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (0.40.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (56.0.0)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.26.3)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchmetrics[image] in ./venv/lib/python3.8/site-packages (0.11.4)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (4.5.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (2.0.0)\n",
      "Requirement already satisfied: torchvision>=0.8 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (0.15.1)\n",
      "Requirement already satisfied: lpips<=0.1.4 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (0.1.4)\n",
      "Requirement already satisfied: scipy>1.0.0 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (1.10.1)\n",
      "Requirement already satisfied: torch-fidelity<=0.3.0 in ./venv/lib/python3.8/site-packages (from torchmetrics[image]) (0.3.0)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in ./venv/lib/python3.8/site-packages (from lpips<=0.1.4->torchmetrics[image]) (4.65.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.7.99)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (2.14.3)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (2.0.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (3.11.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch>=1.8.1->torchmetrics[image]) (3.1.2)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics[image]) (0.40.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics[image]) (56.0.0)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[image]) (3.26.3)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics[image]) (16.0.1)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.8/site-packages (from torch-fidelity<=0.3.0->torchmetrics[image]) (9.5.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from torchvision>=0.8->torchmetrics[image]) (2.28.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch>=1.8.1->torchmetrics[image]) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests->torchvision>=0.8->torchmetrics[image]) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch>=1.8.1->torchmetrics[image]) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/home/j/Desktop/Programming/DeepLearning/GANs/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load celebA dataset\n",
    "def get_dataloader(batch_size, shuffle, n_workers, image_size):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(image_size),\n",
    "            torchvision.transforms.CenterCrop(image_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    dataset = torchvision.datasets.CelebA(\n",
    "        root=\"./data\", transform=transform, download=True\n",
    "    )\n",
    "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(128)])\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionConv(nn.Module):\n",
    "    def __init__(self, in_d, downscale_factor=8):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Downscale factor is suggested in the paper to reduce memory consumption\n",
    "        they use 8, but 4 or 2 can be used with enough gpu memory\n",
    "        \"\"\"\n",
    "        self.downscale_factor = downscale_factor\n",
    "        self.k_conv = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        )\n",
    "        self.q_conv = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        )\n",
    "        self.v_conv = nn.utils.spectral_norm(nn.Conv2d(in_d, in_d, 1, 1, 0))\n",
    "        # gamma is a learnable parameter (used in original paper)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.utils.spectral_norm(nn.Conv2d(in_d, in_d, 1, 1, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_d, h, w)\n",
    "        \"\"\"\n",
    "        batch_size, in_d, h, w = x.shape\n",
    "        # Embed input and reshape for matrix multiplication\n",
    "        k = self.k_conv(x).view(batch_size, -1, h * w)\n",
    "        q = self.q_conv(x).view(batch_size, -1, h * w)\n",
    "        v = self.v_conv(x).view(batch_size, -1, h * w)\n",
    "        # (batch_size, h * w, h * w)\n",
    "        attn = torch.bmm(k.transpose(-2, -1), q)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # (batch_size, in_d, h * w)\n",
    "        out = torch.bmm(v, attn.transpose(-2, -1))\n",
    "        out = out.view(batch_size, in_d, h, w)\n",
    "        out = self.out_conv(out)\n",
    "        # Add residual connection and scale by learnable parameter gamma\n",
    "        out = self.gamma * out + x\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "# Generator, DC-GAN with self attention\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(noise_dim, emb_dim * 8, 4)),\n",
    "            nn.BatchNorm2d(emb_dim * 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(emb_dim * 8, emb_dim * 4, 4, 2, 1)\n",
    "            ),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(emb_dim * 4, emb_dim * 2, 4, 2, 1)\n",
    "            ),\n",
    "            nn.BatchNorm2d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(emb_dim * 2, emb_dim, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer_5 = nn.Sequential(nn.ConvTranspose2d(emb_dim, 3, 4, 2, 1), nn.Tanh())\n",
    "\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim * 2)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.layer_1(noise)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x, attn1 = self.self_attn_1(x)\n",
    "        x = self.layer_4(x)\n",
    "        x, attn2 = self.self_attn_2(x)\n",
    "        x = self.layer_5(x)\n",
    "        return x, attn1, attn2\n",
    "\n",
    "\n",
    " # Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_d=3, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(in_d, emb_dim * 2, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 2, emb_dim * 4, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 4, emb_dim * 4, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 4, emb_dim * 8, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_5 = nn.Conv2d(emb_dim * 8, 1, 4, 1, 0)\n",
    "\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim * 4)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim * 8)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.layer_1(image)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x, attn1 = self.self_attn_1(x)\n",
    "        x = self.layer_4(x)\n",
    "        x, attn2 = self.self_attn_2(x)\n",
    "        x = self.layer_5(x)\n",
    "        return x, attn1, attn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        g_lr: float,\n",
    "        d_lr: float,\n",
    "        # Model parameters\n",
    "        noise_d: int,\n",
    "        output_image_dims: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "        adv_loss: str = \"hinge\",\n",
    "        ckpt_path: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        n_log_steps: Logs every n_log_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        noise_d: Noise dimension\n",
    "        emb_d: Embedding dimension\n",
    "        output_image_dims: Output size ie: height and width of the image\n",
    "        flush_prev_logs: Flushes previous logs and checkpoints\n",
    "        \"\"\"\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # Initialize models\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = Generator().to(self.device)\n",
    "        self.discriminator = Discriminator().to(self.device)\n",
    "        if ckpt_path:\n",
    "            print(f\"Loading checkpoint from {ckpt_path}\")\n",
    "            self.generator.load_state_dict(torch.load(ckpt_path)[\"generator\"])\n",
    "            self.discriminator.load_state_dict(torch.load(ckpt_path)[\"discriminator\"])\n",
    "\n",
    "        # Print the parameter count for each model\n",
    "        print(\n",
    "            f\"Generator has {self.count_parameters(self.generator):,} trainable parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Discriminator has {self.count_parameters(self.discriminator):,} trainable parameters\"\n",
    "        )\n",
    "        # Initialize optimisers\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=g_lr,\n",
    "        )\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=d_lr,\n",
    "        )\n",
    "        self.train_dataloader = get_dataloader(batch_size, True, 4, output_image_dims)\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_d = noise_d\n",
    "        self.adv_loss = adv_loss\n",
    "        # Init globals & Metrics\n",
    "        self.global_step = 0\n",
    "        self.inception = InceptionScore(normalize=True)\n",
    "        self.fid = FrechetInceptionDistance(normalize=True)\n",
    "        self.g_losses, self.d_losses = [], []\n",
    "        # Reset logs\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "        # Create log directory\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        self.log_dir = f'results/logs/train-run-{len(os.listdir(\"results/logs\")) + 1}'\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/metrics\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/checkpoints\", exist_ok=True)\n",
    "        print(f\"Logging to {self.log_dir}\")\n",
    "\n",
    "    def reset_grads(self):\n",
    "        self.optim_G.zero_grad()\n",
    "        self.optim_D.zero_grad()\n",
    "\n",
    "    def count_parameters(self, model: nn.Module):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "        # Save final model\n",
    "        torch.save(self.generator.state_dict(), f\"{self.log_dir}/generator.pt\")\n",
    "        # Save losses\n",
    "        torch.save(self.g_losses, f\"{self.log_dir}/g_losses.pt\")\n",
    "        torch.save(self.d_losses, f\"{self.log_dir}/d_losses.pt\")\n",
    "        self.display_metrics(f\"{self.log_dir}/metrics\")\n",
    "\n",
    "    def train_iter(self, epoch: int):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        # Train on a batch of images\n",
    "        for i, (real_img, tags) in enumerate(\n",
    "            tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            real_img, tags = real_img.to(self.device), tags.to(self.device)\n",
    "            # Train the discriminator\n",
    "            noise = torch.randn((self.batch_size, self.noise_d, 1, 1)).to(self.device)\n",
    "            fake_image, _, _ = self.generator(noise)\n",
    "            loss_D = self.discriminator_step(real_img, fake_image)\n",
    "            # Train the generators\n",
    "            loss_G = self.generator_step()\n",
    "\n",
    "            # Log the losses & Metrics\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                self.g_losses.append(loss_G.item())\n",
    "                self.d_losses.append(loss_D.item())\n",
    "                self.log_image(real_img, f\"{self.global_step}_real_image.png\")\n",
    "                self.log_image(\n",
    "                    self.generator(noise)[0], f\"{self.global_step}_fake_image.png\"\n",
    "                )\n",
    "                # Compute inception score\n",
    "                self.inception.update((self.generator(noise)[0].detach().cpu()))\n",
    "                # Compute FID\n",
    "                self.fid.update(self.generator(noise)[0].detach().cpu(), False)\n",
    "                self.fid.update(real_img.cpu(), True)\n",
    "                # Compute discriminator accuracy\n",
    "                real_pred_D, _, _ = self.discriminator(real_img)\n",
    "                fake_pred_D, _, _ = self.discriminator(fake_image)\n",
    "                real_acc = ((real_pred_D > 0.5).float().mean()).item()\n",
    "                fake_acc = ((fake_pred_D < 0.5).float().mean()).item()\n",
    "                # Log the metrics\n",
    "                self.log_metrics(\n",
    "                    {\n",
    "                        \"Generator Loss\": loss_G.item(),\n",
    "                        \"Discriminator Loss\": loss_D.item(),\n",
    "                        \"Inception Score\": self.inception.compute(),\n",
    "                        \"FID\": self.fid.compute().item(),\n",
    "                        \"Real Accuracy\": real_acc,\n",
    "                        \"Fake Accuracy\": fake_acc,\n",
    "                    }\n",
    "                )\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            # Save a checkpoint\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def discriminator_step(self, real_img, generated_img):\n",
    "        if self.adv_loss == \"wgan\":\n",
    "            # Compute the discriminator loss\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            d_loss = -torch.mean(real_pred_D) + torch.mean(fake_pred_D)\n",
    "            self.reset_grads()\n",
    "            d_loss.backward()\n",
    "            self.optim_D.step()\n",
    "\n",
    "            # Compute the gradient penalty note that the code for calculating gradient penalty is inspired by this tensorflow repo:\n",
    "            # https://github.com/taki0112/Self-Attention-GAN-Tensorflow/blob/6c073a4c8bf9898ab3ed9b470451f5630cd05373/SAGAN.py#L6\n",
    "            alpha = (\n",
    "                torch.rand(real_img.size(0), 1, 1, 1)\n",
    "                .to(self.device)\n",
    "                .expand_as(real_img)\n",
    "            )\n",
    "            interpolated = Variable(\n",
    "                (alpha * real_img) + (1 - alpha) * real_img, requires_grad=True\n",
    "            )\n",
    "            out, _, _ = self.discriminator(interpolated)\n",
    "\n",
    "            grad = torch.autograd.grad(\n",
    "                outputs=out,\n",
    "                inputs=interpolated,\n",
    "                grad_outputs=torch.ones(out.size()).to(self.device),\n",
    "                retain_graph=True,\n",
    "                create_graph=True,\n",
    "                only_inputs=True,\n",
    "            )[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_norm = torch.sqrt(torch.sum(grad**2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_norm - 1) ** 2)\n",
    "            # Backward + Optimize\n",
    "            d_loss = 10 * d_loss_gp\n",
    "            self.reset_grads()\n",
    "            d_loss.backward()\n",
    "            self.optim_D.step()\n",
    "            return d_loss\n",
    "\n",
    "        elif self.adv_loss == \"hinge\":\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            loss_D = F.relu(1 - real_pred_D).mean() + F.relu(1 + fake_pred_D).mean()\n",
    "            return loss_D\n",
    "        elif self.adv_loss == \"lsgan\":\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            real_loss = F.mse_loss(real_pred_D, torch.ones_like(real_pred_D))\n",
    "            fake_loss = F.mse_loss(fake_pred_D, torch.zeros_like(fake_pred_D))\n",
    "            return real_loss + fake_loss\n",
    "\n",
    "    # For generator loss\n",
    "    def generator_step(self):\n",
    "        noise_x = torch.randn(self.batch_size, self.noise_d, 1, 1).to(self.device)\n",
    "        fake_image, _, _ = self.generator(noise_x)\n",
    "        fake_pred_D, _, _ = self.discriminator(fake_image)\n",
    "        g_loss = -fake_pred_D.mean()\n",
    "        self.reset_grads()\n",
    "        g_loss.backward()\n",
    "        self.optim_G.step()\n",
    "        return g_loss\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"generator\": self.generator.state_dict(),\n",
    "                \"discriminator\": self.discriminator.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                \"optim_D\": self.optim_D.state_dict(),\n",
    "            },\n",
    "            f\"{self.log_dir}/checkpoints/step-{self.global_step}.pt\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        state_dict = torch.load(path)\n",
    "        self.generator.load_state_dict(state_dict[\"generator\"])\n",
    "        self.discriminator.load_state_dict(state_dict[\"discriminator\"])\n",
    "        self.optim_G.load_state_dict(state_dict[\"optim_G\"])\n",
    "        self.optim_D.load_state_dict(state_dict[\"optim_D\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            f\"{self.log_dir}/images/{name}\",\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, metrics: Dict[str, Any]):\n",
    "        torch.save(\n",
    "            metrics, f\"{self.log_dir}/metrics/step-{self.global_step}-metrics.pt\"\n",
    "        )\n",
    "\n",
    "    def display_metrics(self, metrics_dir: str):\n",
    "        g_loss, d_loss = [], []\n",
    "        real_acc, fake_acc = [], []\n",
    "        for file in natsorted(os.listdir(metrics_dir)):\n",
    "            metrics = torch.load(os.path.join(metrics_dir, file))\n",
    "            g_loss.append(metrics[\"Generator Loss\"])\n",
    "            d_loss.append(metrics[\"Discriminator Loss\"])\n",
    "            real_acc.append(metrics[\"Real Accuracy\"])\n",
    "            fake_acc.append(metrics[\"Fake Accuracy\"])\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "        ax[0, 0].plot(g_loss)\n",
    "        ax[0, 0].set_title(\"Generator Loss\")\n",
    "        ax[0, 1].plot(d_loss)\n",
    "        ax[0, 1].set_title(\"Discriminator Loss\")\n",
    "        ax[1, 0].plot(real_acc)\n",
    "        ax[1, 0].set_title(\"Real Accuracy\")\n",
    "        ax[1, 1].plot(fake_acc)\n",
    "        ax[1, 1].set_title(\"Fake Accuracy\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator has 3,624,181 trainable parameters\n",
      "Discriminator has 4,426,819 trainable parameters\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to results/logs/train-run-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "  File \"/tmp/ipykernel_55120/3059889046.py\", line 1, in <module>\n",
      "    Trainer(\n",
      "  File \"/tmp/ipykernel_55120/3503854386.py\", line 100, in run\n",
      "    self.train_iter(e)\n",
      "  File \"/tmp/ipykernel_55120/3503854386.py\", line 132, in train_iter\n",
      "    self.inception.update((self.generator(noise)[0].detach().cpu()))\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torchmetrics/metric.py\", line 390, in wrapped_func\n",
      "    update(*args, **kwargs)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torchmetrics/image/inception.py\", line 139, in update\n",
      "    features = self.inception(imgs)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return handle\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torchmetrics/image/fid.py\", line 57, in forward\n",
      "    out = super().forward(x)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch_fidelity/feature_extractor_inceptionv3.py\", line 127, in forward\n",
      "    x = self.Conv2d_4a_3x3(x)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return handle\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch_fidelity/feature_extractor_inceptionv3.py\", line 208, in forward\n",
      "    x = self.conv(x)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return handle\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/pygments/styles/__init__.py\", line 82, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 844, in get_records\n",
      "  File \"/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/pygments/styles/__init__.py\", line 84, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "Trainer(\n",
    "    n_ckpt_steps=5000,\n",
    "    n_log_steps=750,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    g_lr=0.0001,\n",
    "    d_lr=0.0004,\n",
    "    noise_d=100,\n",
    "    output_image_dims=64,  # H x W\n",
    "    flush_prev_logs=False,\n",
    "    adv_loss=\"wgan\",\n",
    "    ckpt_path=None,\n",
    ").run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceEngine:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.generator = Generator()\n",
    "        self.generator.load_state_dict(torch.load(model_path)[\"generator\"])\n",
    "\n",
    "    def generate(self, output_path: str = \"results/out.png\"):\n",
    "        noise = torch.randn((1, 100, 1, 1))\n",
    "        image, _, _ = self.generator(noise)\n",
    "        # Make directory to save to\n",
    "\n",
    "        torchvision.utils.save_image(image, output_path, normalize=True, range=(-1, 1))\n",
    "        print(f\"Generated image saved at {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd50f146da535ab3830ed5f78683553ef57acd3032c46b9f95757dd92f9c813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
