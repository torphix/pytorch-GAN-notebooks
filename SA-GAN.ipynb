{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load celebA dataset\n",
    "def get_dataloader(batch_size, shuffle, n_workers, image_size):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_size),\n",
    "        torchvision.transforms.CenterCrop(image_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.CelebA(root='./data', transform=transform, download=True)\n",
    "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(10)])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionConv(nn.Module):\n",
    "    def __init__(self, in_d, downscale_factor=8):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Downscale factor is suggested in the paper to reduce memory consumption\n",
    "        they use 8, but you can use 4 or 2 if you have enough memory\n",
    "        \"\"\"\n",
    "        self.downscale_factor = downscale_factor\n",
    "        self.k_conv = nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        self.q_conv = nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        self.v_conv = nn.Conv2d(in_d, in_d, 1, 1, 0)\n",
    "        # gamma is a learnable parameter (used in original paper)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.Conv2d(in_d, in_d, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_d, h, w)\n",
    "        \"\"\"\n",
    "        batch_size, in_d, h, w = x.shape\n",
    "        # Embed input and reshape for matrix multiplication\n",
    "        k = self.k_conv(x).view(batch_size, -1, h * w)\n",
    "        q = self.q_conv(x).view(batch_size, -1, h * w)\n",
    "        v = self.v_conv(x).view(batch_size, -1, h * w)\n",
    "        # (batch_size, h * w, h * w)\n",
    "        attn = torch.bmm(k.transpose(-2, -1), q)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # (batch_size, in_d, h * w)\n",
    "        out = torch.bmm(v, attn.transpose(-2, -1))\n",
    "        out = out.view(batch_size, in_d, h, w)\n",
    "        out = self.out_conv(out)\n",
    "        # Add residual connection and scale by learnable parameter gamma\n",
    "        out = self.gamma * out + x\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "# Generator, DC-GAN with self attention\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, target_output_size=64, emb_dim=512):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_dim, emb_dim, 4),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        n_layers = int(np.log2(target_output_size))\n",
    "        for _ in range(n_layers-3):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        emb_dim,\n",
    "                        emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            # Don't shrink emb_dim below too low\n",
    "            if emb_dim // 2 >= 64:\n",
    "                emb_dim = emb_dim // 2\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        # Self attention sandwhiches the penultimate layer\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "        self.penultimate_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(emb_dim, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(nn.ConvTranspose2d(emb_dim, 3, 4, 2, 1), nn.Tanh())\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.in_layer(noise)\n",
    "        # Main layers\n",
    "        out = self.layers(out)\n",
    "        # Attention layers\n",
    "        out, attn1 = self.self_attn_1(out)\n",
    "        out = self.penultimate_layer(out)\n",
    "        out, attn2 = self.self_attn_2(out)\n",
    "        # Out layers\n",
    "        out = self.out_layer(out)\n",
    "        return out, attn1, attn2\n",
    "\n",
    "\n",
    "# # Discriminator, adopts the PatchGAN architecture ie: output is a receptive field of n x n\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_d=3, emb_dim=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_d, emb_dim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        emb_dim,\n",
    "                        emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(emb_dim // 2 if emb_dim // 2 >= 64 else emb_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            # Don't shrink emb_dim below too low\n",
    "            if emb_dim // 2 >= 64:\n",
    "                emb_dim = emb_dim // 2\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        # Self attention sandwhiches the penultimate layer\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "        self.penultimate_layer = nn.Sequential(\n",
    "            nn.Conv2d(emb_dim, emb_dim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Conv2d(emb_dim, 1, 4)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.in_layer(noise)\n",
    "        # Main layers\n",
    "        out = self.layers(out)\n",
    "        # Attention layers\n",
    "        out, attn1 = self.self_attn_1(out)\n",
    "        out = self.penultimate_layer(out)\n",
    "        out, attn2 = self.self_attn_2(out)\n",
    "        # Out layers\n",
    "        out = self.out_layer(out)\n",
    "        return out, attn1, attn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        lr: float,\n",
    "        # Model parameters\n",
    "        noise_d: int,\n",
    "        emb_d: int,\n",
    "        output_size: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        data_dir: Directory containing the data, must contain trainA and trainB folders\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta_1: Beta 1 for Adam optimiser\n",
    "        beta_2: Beta 2 for Adam optimiser\n",
    "        \"\"\"\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "            shutil.rmtree(\"results/checkpoints\", ignore_errors=True)\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        os.makedirs(\"results/checkpoints\", exist_ok=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = Generator(\n",
    "            # noise_dim=noise_d, emb_dim=emb_d, target_output_size=output_size\n",
    "        ).to(self.device)\n",
    "        self.discriminator = Discriminator().to(self.device)\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=lr,\n",
    "        )\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=lr,\n",
    "        )\n",
    "        self.train_dataloader = get_dataloader(batch_size, True, 4, output_size)\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_d = noise_d\n",
    "        # Init variables\n",
    "        self.global_step = 0\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "\n",
    "    def train_iter(self, epoch: int):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        # Train on a batch of images\n",
    "        for i, (real_img, tags) in enumerate(\n",
    "            tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            real_img, tags = real_img.to(self.device), tags.to(self.device)\n",
    "            # Train the discriminator\n",
    "            self.optim_D.zero_grad()\n",
    "            noise = torch.randn((self.batch_size, 100, 1, 1)).to(self.device)\n",
    "            fake_image, _, _ = self.generator(noise)\n",
    "            loss_D = self.compute_discriminator_loss(real_img, fake_image)\n",
    "            loss_D.backward()\n",
    "            self.optim_D.step()\n",
    "            # Train the generators\n",
    "            self.optim_G.zero_grad()\n",
    "            loss_G = self.compute_generator_loss()\n",
    "            loss_G.backward()\n",
    "            self.optim_G.step()\n",
    "            # Log the losses\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                self.log_image(real_img, f\"{self.global_step}_real_image.png\")\n",
    "                self.log_image(\n",
    "                    self.generator(noise)[0], f\"{self.global_step}_fake_image.png\"\n",
    "                )\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "        print(loss_G, loss_D, self.global_step)\n",
    "\n",
    "    def compute_generator_loss(self) -> torch.Tensor:\n",
    "        noise_x = torch.randn(self.batch_size, self.noise_d, 1, 1).to(self.device)\n",
    "        fake_image, _, _ = self.generator(noise_x)\n",
    "        fake_pred_D, _, _ = self.discriminator(fake_image)\n",
    "        # Hinge loss as specified in the paper\n",
    "        loss_G = -fake_pred_D.mean()\n",
    "        return loss_G\n",
    "\n",
    "    def compute_discriminator_loss(\n",
    "        self,\n",
    "        real_img: torch.Tensor,\n",
    "        generated_img: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        real_pred_D, _, _ = self.discriminator(real_img)\n",
    "        fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "        # Hinge loss as specified in the paper\n",
    "        loss_D = F.relu(1 - real_pred_D).mean() + F.relu(1 + fake_pred_D).mean()\n",
    "        return loss_D\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"generator\": self.generator.state_dict(),\n",
    "                \"discriminator\": self.discriminator.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                \"optim_D\": self.optim_D.state_dict(),\n",
    "            },\n",
    "            f\"results/checkpoints/{self.global_step}.pt\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        state_dict = torch.load(path)\n",
    "        self.generator.load_state_dict(state_dict[\"generator\"])\n",
    "        self.discriminator.load_state_dict(state_dict[\"discriminator\"])\n",
    "        self.optim_G.load_state_dict(state_dict[\"optim_G\"])\n",
    "        self.optim_D.load_state_dict(state_dict[\"optim_D\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            \"results/logs\" + name,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0260, device='cuda:0', grad_fn=<NegBackward0>) tensor(0., device='cuda:0', grad_fn=<AddBackward0>) 5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:  37%|███▋      | 1891/5087 [02:09<03:42, 14.37it/s]"
     ]
    }
   ],
   "source": [
    "Trainer(\n",
    "    n_ckpt_steps=5000,\n",
    "    n_log_steps=1000,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    lr=0.0001,\n",
    "    noise_d=100,\n",
    "    emb_d=512,\n",
    "    output_size=64,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31mError executing Jupyter command 'notebook': [Errno 2] No such file or directory. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# - Add spectral norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd50f146da535ab3830ed5f78683553ef57acd3032c46b9f95757dd92f9c813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
