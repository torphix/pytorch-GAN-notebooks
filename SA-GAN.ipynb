{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load celebA dataset\n",
    "def get_dataloader(batch_size, shuffle, n_workers, image_size):\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(image_size),\n",
    "            torchvision.transforms.CenterCrop(image_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    dataset = torchvision.datasets.CelebA(\n",
    "        root=\"./data\", transform=transform, download=True\n",
    "    )\n",
    "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(128)])\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionConv(nn.Module):\n",
    "    def __init__(self, in_d, downscale_factor=8):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Downscale factor is suggested in the paper to reduce memory consumption\n",
    "        they use 8, but 4 or 2 can be used with enough gpu memory\n",
    "        \"\"\"\n",
    "        self.downscale_factor = downscale_factor\n",
    "        self.k_conv = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        )\n",
    "        self.q_conv = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(in_d, in_d // self.downscale_factor, 1, 1, 0)\n",
    "        )\n",
    "        self.v_conv = nn.utils.spectral_norm(nn.Conv2d(in_d, in_d, 1, 1, 0))\n",
    "        # gamma is a learnable parameter (used in original paper)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.out_conv = nn.utils.spectral_norm(nn.Conv2d(in_d, in_d, 1, 1, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_d, h, w)\n",
    "        \"\"\"\n",
    "        batch_size, in_d, h, w = x.shape\n",
    "        # Embed input and reshape for matrix multiplication\n",
    "        k = self.k_conv(x).view(batch_size, -1, h * w)\n",
    "        q = self.q_conv(x).view(batch_size, -1, h * w)\n",
    "        v = self.v_conv(x).view(batch_size, -1, h * w)\n",
    "        # (batch_size, h * w, h * w)\n",
    "        attn = torch.bmm(k.transpose(-2, -1), q)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # (batch_size, in_d, h * w)\n",
    "        out = torch.bmm(v, attn.transpose(-2, -1))\n",
    "        out = out.view(batch_size, in_d, h, w)\n",
    "        out = self.out_conv(out)\n",
    "        # Add residual connection and scale by learnable parameter gamma\n",
    "        out = self.gamma * out + x\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "# Generator, DC-GAN with self attention\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(noise_dim, emb_dim * 8, 4)),\n",
    "            nn.BatchNorm2d(emb_dim * 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(emb_dim * 8, emb_dim * 4, 4, 2, 1)\n",
    "            ),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(emb_dim * 4, emb_dim * 2, 4, 2, 1)\n",
    "            ),\n",
    "            nn.BatchNorm2d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(emb_dim * 2, emb_dim, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer_5 = nn.Sequential(nn.ConvTranspose2d(emb_dim, 3, 4, 2, 1), nn.Tanh())\n",
    "\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim * 2)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.layer_1(noise)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x, attn1 = self.self_attn_1(x)\n",
    "        x = self.layer_4(x)\n",
    "        x, attn2 = self.self_attn_2(x)\n",
    "        x = self.layer_5(x)\n",
    "        return x, attn1, attn2\n",
    "\n",
    "\n",
    " # Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_d=3, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(in_d, emb_dim * 2, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 2, emb_dim * 4, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 4, emb_dim * 4, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(emb_dim * 4, emb_dim * 8, 4, 2, 1)),\n",
    "            nn.BatchNorm2d(emb_dim * 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_5 = nn.Conv2d(emb_dim * 8, 1, 4, 1, 0)\n",
    "\n",
    "        self.self_attn_1 = SelfAttentionConv(emb_dim * 4)\n",
    "        self.self_attn_2 = SelfAttentionConv(emb_dim * 8)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = self.layer_1(image)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x, attn1 = self.self_attn_1(x)\n",
    "        x = self.layer_4(x)\n",
    "        x, attn2 = self.self_attn_2(x)\n",
    "        x = self.layer_5(x)\n",
    "        return x, attn1, attn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ckpt_steps: int,\n",
    "        n_log_steps: int,\n",
    "        epochs: int,\n",
    "        # Data parameters\n",
    "        batch_size: int,\n",
    "        # Optimiser parameters\n",
    "        g_lr: float,\n",
    "        d_lr: float,\n",
    "        # Model parameters\n",
    "        noise_d: int,\n",
    "        output_image_dims: int,\n",
    "        flush_prev_logs: bool = True,\n",
    "        adv_loss: str = \"hinge\",\n",
    "        ckpt_path: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_ckpt_steps: Saves a checkpoint every n_ckpt_steps\n",
    "        n_log_steps: Logs every n_log_steps\n",
    "        epochs: Number of epochs to train for\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        noise_d: Noise dimension\n",
    "        emb_d: Embedding dimension\n",
    "        output_image_dims: Output size ie: height and width of the image\n",
    "        flush_prev_logs: Flushes previous logs and checkpoints\n",
    "        \"\"\"\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # Initialize models\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = Generator(\n",
    "            # noise_dim=noise_d,\n",
    "            # emb_dim=emb_d,\n",
    "            # output_image_dims=output_image_dims,\n",
    "        ).to(self.device)\n",
    "        self.discriminator = Discriminator(\n",
    "            # emb_dim=emb_d, n_layers=self.generator.n_layers\n",
    "        ).to(self.device)\n",
    "        if ckpt_path:\n",
    "            print(f\"Loading checkpoint from {ckpt_path}\")\n",
    "            self.generator.load_state_dict(torch.load(ckpt_path)[\"generator\"])\n",
    "            self.discriminator.load_state_dict(torch.load(ckpt_path)[\"discriminator\"])\n",
    "\n",
    "        # Print the parameter count for each model\n",
    "        print(\n",
    "            f\"Generator has {self.count_parameters(self.generator):,} trainable parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Discriminator has {self.count_parameters(self.discriminator):,} trainable parameters\"\n",
    "        )\n",
    "        # Initialize optimisers\n",
    "        self.optim_G = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=g_lr,\n",
    "        )\n",
    "        self.optim_D = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=d_lr,\n",
    "        )\n",
    "        self.train_dataloader = get_dataloader(batch_size, True, 4, output_image_dims)\n",
    "        # Hyperparameters\n",
    "        self.n_log_steps = n_log_steps\n",
    "        self.n_ckpt_steps = n_ckpt_steps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_d = noise_d\n",
    "        self.adv_loss = adv_loss\n",
    "        # Init globals & Metrics\n",
    "        self.global_step = 0\n",
    "        self.inception = InceptionScore(normalize=True)\n",
    "        self.fid = FrechetInceptionDistance(normalize=True)\n",
    "        self.g_losses, self.d_losses = [], []\n",
    "        # Reset logs\n",
    "        if flush_prev_logs:\n",
    "            shutil.rmtree(\"results/logs\", ignore_errors=True)\n",
    "        # Create log directory\n",
    "        os.makedirs(\"results/logs\", exist_ok=True)\n",
    "        self.log_dir = f'results/logs/train-run-{len(os.listdir(\"results/logs\")) + 1}'\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/images\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/metrics\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.log_dir}/checkpoints\", exist_ok=True)\n",
    "        print(f\"Logging to {self.log_dir}\")\n",
    "\n",
    "    def reset_grads(self):\n",
    "        self.optim_G.zero_grad()\n",
    "        self.optim_D.zero_grad()\n",
    "\n",
    "    def count_parameters(self, model: nn.Module):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def to_device(self, *args):\n",
    "        return [arg.to(self.device) for arg in args]\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.epochs):\n",
    "            self.train_iter(e)\n",
    "        # Save final model\n",
    "        torch.save(self.generator.state_dict(), f\"{self.log_dir}/generator.pt\")\n",
    "        # Save losses\n",
    "        torch.save(self.g_losses, f\"{self.log_dir}/g_losses.pt\")\n",
    "        torch.save(self.d_losses, f\"{self.log_dir}/d_losses.pt\")\n",
    "        self.display_metrics(f\"{self.log_dir}/metrics\")\n",
    "\n",
    "    def train_iter(self, epoch: int):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        # Train on a batch of images\n",
    "        for i, (real_img, tags) in enumerate(\n",
    "            tqdm(self.train_dataloader, desc=f\"Epoch: {epoch}\", leave=False)\n",
    "        ):\n",
    "            real_img, tags = real_img.to(self.device), tags.to(self.device)\n",
    "            # Train the discriminator\n",
    "            noise = torch.randn((self.batch_size, self.noise_d, 1, 1)).to(self.device)\n",
    "            fake_image, _, _ = self.generator(noise)\n",
    "            loss_D = self.discriminator_step(real_img, fake_image)\n",
    "            # Train the generators\n",
    "            loss_G = self.generator_step()\n",
    "\n",
    "            # Log the losses & Metrics\n",
    "            if self.global_step % self.n_log_steps == 0:\n",
    "                self.g_losses.append(loss_G.item())\n",
    "                self.d_losses.append(loss_D.item())\n",
    "                self.log_image(real_img, f\"{self.global_step}_real_image.png\")\n",
    "                self.log_image(\n",
    "                    self.generator(noise)[0], f\"{self.global_step}_fake_image.png\"\n",
    "                )\n",
    "                # Compute inception score\n",
    "                self.inception.update((self.generator(noise)[0].detach().cpu()))\n",
    "                # Compute FID\n",
    "                self.fid.update(self.generator(noise)[0].detach().cpu(), False)\n",
    "                self.fid.update(real_img.cpu(), True)\n",
    "                # Compute discriminator accuracy\n",
    "                real_pred_D, _, _ = self.discriminator(real_img)\n",
    "                fake_pred_D, _, _ = self.discriminator(fake_image)\n",
    "                real_acc = ((real_pred_D > 0.5).float().mean()).item()\n",
    "                fake_acc = ((fake_pred_D < 0.5).float().mean()).item()\n",
    "                # Log the metrics\n",
    "                self.log_metrics(\n",
    "                    {\n",
    "                        \"Generator Loss\": loss_G.item(),\n",
    "                        \"Discriminator Loss\": loss_D.item(),\n",
    "                        \"Inception Score\": self.inception.compute(),\n",
    "                        \"FID\": self.fid.compute().item(),\n",
    "                        \"Real Accuracy\": real_acc,\n",
    "                        \"Fake Accuracy\": fake_acc,\n",
    "                    }\n",
    "                )\n",
    "            # Increment the global step\n",
    "            self.global_step += 1\n",
    "            # Save a checkpoint\n",
    "            if self.global_step % self.n_ckpt_steps == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "    def discriminator_step(self, real_img, generated_img):\n",
    "        if self.adv_loss == \"wgan\":\n",
    "            # Compute the discriminator loss\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            d_loss = -torch.mean(real_pred_D) + torch.mean(fake_pred_D)\n",
    "            self.reset_grads()\n",
    "            d_loss.backward()\n",
    "            self.optim_D.step()\n",
    "\n",
    "            # Compute the gradient penalty\n",
    "            alpha = (\n",
    "                torch.rand(real_img.size(0), 1, 1, 1)\n",
    "                .to(self.device)\n",
    "                .expand_as(real_img)\n",
    "            )\n",
    "            interpolated = Variable(\n",
    "                (alpha * real_img) + (1 - alpha) * real_img, requires_grad=True\n",
    "            )\n",
    "            out, _, _ = self.discriminator(interpolated)\n",
    "\n",
    "            grad = torch.autograd.grad(\n",
    "                outputs=out,\n",
    "                inputs=interpolated,\n",
    "                grad_outputs=torch.ones(out.size()).to(self.device),\n",
    "                retain_graph=True,\n",
    "                create_graph=True,\n",
    "                only_inputs=True,\n",
    "            )[0]\n",
    "\n",
    "            grad = grad.view(grad.size(0), -1)\n",
    "            grad_norm = torch.sqrt(torch.sum(grad**2, dim=1))\n",
    "            d_loss_gp = torch.mean((grad_norm - 1) ** 2)\n",
    "            # Backward + Optimize\n",
    "            d_loss = 10 * d_loss_gp\n",
    "            self.reset_grads()\n",
    "            d_loss.backward()\n",
    "            self.optim_D.step()\n",
    "            return d_loss\n",
    "\n",
    "        elif self.adv_loss == \"hinge\":\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            loss_D = F.relu(1 - real_pred_D).mean() + F.relu(1 + fake_pred_D).mean()\n",
    "            return loss_D\n",
    "        elif self.adv_loss == \"lsgan\":\n",
    "            real_pred_D, _, _ = self.discriminator(real_img)\n",
    "            fake_pred_D, _, _ = self.discriminator(generated_img)\n",
    "            real_loss = F.mse_loss(real_pred_D, torch.ones_like(real_pred_D))\n",
    "            fake_loss = F.mse_loss(fake_pred_D, torch.zeros_like(fake_pred_D))\n",
    "            return real_loss + fake_loss\n",
    "\n",
    "    # For generator loss\n",
    "    def generator_step(self):\n",
    "        noise_x = torch.randn(self.batch_size, self.noise_d, 1, 1).to(self.device)\n",
    "        fake_image, _, _ = self.generator(noise_x)\n",
    "        fake_pred_D, _, _ = self.discriminator(fake_image)\n",
    "        g_loss = -fake_pred_D.mean()\n",
    "        self.reset_grads()\n",
    "        g_loss.backward()\n",
    "        self.optim_G.step()\n",
    "        return g_loss\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"generator\": self.generator.state_dict(),\n",
    "                \"discriminator\": self.discriminator.state_dict(),\n",
    "                \"optim_G\": self.optim_G.state_dict(),\n",
    "                \"optim_D\": self.optim_D.state_dict(),\n",
    "            },\n",
    "            f\"{self.log_dir}/checkpoints/step-{self.global_step}.pt\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        state_dict = torch.load(path)\n",
    "        self.generator.load_state_dict(state_dict[\"generator\"])\n",
    "        self.discriminator.load_state_dict(state_dict[\"discriminator\"])\n",
    "        self.optim_G.load_state_dict(state_dict[\"optim_G\"])\n",
    "        self.optim_D.load_state_dict(state_dict[\"optim_D\"])\n",
    "\n",
    "    def log_image(self, img: torch.Tensor, name: str):\n",
    "        torchvision.utils.save_image(\n",
    "            img,\n",
    "            f\"{self.log_dir}/images/{name}\",\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, metrics: Dict[str, Any]):\n",
    "        torch.save(\n",
    "            metrics, f\"{self.log_dir}/metrics/step-{self.global_step}-metrics.pt\"\n",
    "        )\n",
    "\n",
    "    def display_metrics(self, metrics_dir: str):\n",
    "        g_loss, d_loss = [], []\n",
    "        real_acc, fake_acc = [], []\n",
    "        for file in natsorted(os.listdir(metrics_dir)):\n",
    "            metrics = torch.load(os.path.join(metrics_dir, file))\n",
    "            g_loss.append(metrics[\"Generator Loss\"])\n",
    "            d_loss.append(metrics[\"Discriminator Loss\"])\n",
    "            real_acc.append(metrics[\"Real Accuracy\"])\n",
    "            fake_acc.append(metrics[\"Fake Accuracy\"])\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "        ax[0, 0].plot(g_loss)\n",
    "        ax[0, 0].set_title(\"Generator Loss\")\n",
    "        ax[0, 1].plot(d_loss)\n",
    "        ax[0, 1].set_title(\"Discriminator Loss\")\n",
    "        ax[1, 0].plot(real_acc)\n",
    "        ax[1, 0].set_title(\"Real Accuracy\")\n",
    "        ax[1, 1].plot(fake_acc)\n",
    "        ax[1, 1].set_title(\"Fake Accuracy\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator has 3,624,181 trainable parameters\n",
      "Discriminator has 4,426,819 trainable parameters\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to results/logs/train-run-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Trainer(\n\u001b[1;32m      2\u001b[0m     n_ckpt_steps\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     n_log_steps\u001b[39m=\u001b[39;49m\u001b[39m750\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     g_lr\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     d_lr\u001b[39m=\u001b[39;49m\u001b[39m0.0004\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     noise_d\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     output_image_dims\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,  \u001b[39m# H x W\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     flush_prev_logs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     adv_loss\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwgan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     13\u001b[0m )\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_iter(e)\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Save final model\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_dir\u001b[39m}\u001b[39;00m\u001b[39m/generator.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 119\u001b[0m, in \u001b[0;36mTrainer.train_iter\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    117\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnoise_d, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    118\u001b[0m fake_image, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(noise)\n\u001b[0;32m--> 119\u001b[0m loss_D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiscriminator_step(real_img, fake_image)\n\u001b[1;32m    120\u001b[0m \u001b[39m# Train the generators\u001b[39;00m\n\u001b[1;32m    121\u001b[0m loss_G \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator_step()\n",
      "Cell \u001b[0;32mIn[4], line 165\u001b[0m, in \u001b[0;36mTrainer.discriminator_step\u001b[0;34m(self, real_img, generated_img)\u001b[0m\n\u001b[1;32m    163\u001b[0m d_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(real_pred_D) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmean(fake_pred_D)\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_grads()\n\u001b[0;32m--> 165\u001b[0m d_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_D\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    168\u001b[0m \u001b[39m# Compute the gradient penalty\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/GANs/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Trainer(\n",
    "    n_ckpt_steps=5000,\n",
    "    n_log_steps=750,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    g_lr=0.0001,\n",
    "    d_lr=0.0004,\n",
    "    noise_d=100,\n",
    "    output_image_dims=64,  # H x W\n",
    "    flush_prev_logs=False,\n",
    "    adv_loss=\"wgan\",\n",
    "    ckpt_path=None,\n",
    ").run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceEngine:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.generator = Generator()\n",
    "        self.generator.load_state_dict(torch.load(model_path)[\"generator\"])\n",
    "\n",
    "    def generate(self, output_path: str = \"results/out.png\"):\n",
    "        noise = torch.randn((1, 100, 1, 1))\n",
    "        image, _, _ = self.generator(noise)\n",
    "        # Make directory to save to\n",
    "\n",
    "        torchvision.utils.save_image(image, output_path, normalize=True, range=(-1, 1))\n",
    "        print(f\"Generated image saved at {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cd50f146da535ab3830ed5f78683553ef57acd3032c46b9f95757dd92f9c813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
